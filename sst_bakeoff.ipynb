{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bake-off: Stanford Sentiment Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Bake-off submission](#Bake-off-submission)\n",
    "0. [Methodological note](#Methodological-note)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Baseline](#Baseline)\n",
    "0. [TfRNNClassifier wrapper](#TfRNNClassifier-wrapper)\n",
    "0. [TreeNN wrapper](#TreeNN-wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The goal of this in-class bake-off is to __achieve the highest average F1 score__ on the SST development set, with the binary class function.\n",
    "\n",
    "The only restriction: __you cannot make any use of the subtree labels__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off submission\n",
    "\n",
    "1. A description of the model you created.\n",
    "1. The value of `f1-score` in the `avg / total` row of the classification report.\n",
    "\n",
    "Submission URL: https://canvas.stanford.edu/courses/83399/assignments/129640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodological note\n",
    "\n",
    "You don't have to use the experimental framework defined below (based on `sst`). However, if you don't use `sst.experiment` as below, then make sure you're training only on `train`, evaluating on `dev`, and that you report with \n",
    "\n",
    "```\n",
    "from sklearn.metrics import classification_report\n",
    "classification_report(y_dev, predictions)\n",
    "```\n",
    "where `y_dev = [y for tree, y in sst.dev_reader(class_func=sst.binary_class_func)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](sst_01_overview.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from rnn_classifier import RNNClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import sst, utils\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import tensorflow as tf\n",
    "from tf_rnn_classifier import TfRNNClassifier\n",
    "from tree_nn import TreeNN\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(tree):\n",
    "    \"\"\"The basis for a unigrams feature function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : nltk.tree\n",
    "        The tree to represent.\n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    defaultdict\n",
    "        A map from strings to their counts in `tree`. (Counter maps a \n",
    "        list to a dict of counts of the elements in that list.)\n",
    "    \n",
    "    \"\"\"\n",
    "    return Counter(tree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):        \n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.772\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.783     0.741     0.761       428\n",
      "   positive      0.762     0.802     0.782       444\n",
      "\n",
      "avg / total      0.772     0.772     0.772       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    unigrams_phi,                      # Free to write your own!\n",
    "    fit_maxent_classifier,             # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed.\n",
    "    class_func=sst.binary_class_func)  # Fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfRNNClassifier wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_phi(tree):\n",
    "    return tree.leaves()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tf_rnn_classifier(X, y):\n",
    "    vocab = sst.get_vocab(X, n_words=3000)\n",
    "    mod = TfRNNClassifier(\n",
    "        vocab, \n",
    "        eta=0.05,\n",
    "        batch_size=2048,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50,\n",
    "        max_length=52, \n",
    "        max_iter=500,\n",
    "        cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "        hidden_activation=tf.nn.tanh,\n",
    "        train_embedding=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sst.experiment(\n",
    "    rnn_phi,\n",
    "    fit_tf_rnn_classifier, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreeNN wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_phi(tree):\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tree_nn_classifier(X, y):\n",
    "    vocab = sst.get_vocab(X, n_words=3000)\n",
    "    mod = TreeNN(\n",
    "        vocab, \n",
    "        embed_dim=100, \n",
    "        max_iter=100)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b550c7d75fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfit_tree_nn_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# For deep learning, use `vectorize=False`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     assess_reader=sst.dev_reader)\n\u001b[0m",
      "\u001b[0;32m~/Google Drive/*Stanford Notes/CS/224U/Classwork/sst.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(phi, train_func, train_reader, assess_reader, train_size, class_func, score_func, vectorize, verbose)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mX_assess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_assess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# Train:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;31m# Predictions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_assess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d88903ddf12b>\u001b[0m in \u001b[0;36mfit_tree_nn_classifier\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         max_iter=100)\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/*Stanford Notes/CS/224U/Classwork/nn_model_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;31m# Back-prop:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 gradients = self.backward_propagation(\n\u001b[0;32m---> 72\u001b[0;31m                     hidden_states, predictions, ex, labels)\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/*Stanford Notes/CS/224U/Classwork/tree_nn.py\u001b[0m in \u001b[0;36mbackward_propagation\u001b[0;34m(self, vectree, predictions, ex, labels)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0md_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0md_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mh_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_hy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_tanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0md_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md_W_hy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_b_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    rnn_phi,\n",
    "    fit_tree_nn_classifier, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Submission V_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_test(phi,classifier,verbose=True):\n",
    "    print(phi,classifier)\n",
    "    print(\"-----------------------------\")\n",
    "    score = sst.experiment(\n",
    "    phi,                      # Free to write your own!\n",
    "    classifier,             # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed.\n",
    "    class_func=sst.binary_class_func,   # Fixed.\n",
    "    verbose=verbose)  \n",
    "    print(\"-----------------------------\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(\"''\",): 1,\n",
       "         (\"'s\",): 2,\n",
       "         (',',): 1,\n",
       "         ('.',): 1,\n",
       "         ('21st',): 1,\n",
       "         ('<S>',): 1,\n",
       "         ('Arnold',): 1,\n",
       "         ('Century',): 1,\n",
       "         ('Conan',): 1,\n",
       "         ('Damme',): 1,\n",
       "         ('Jean-Claud',): 1,\n",
       "         ('Rock',): 1,\n",
       "         ('Schwarzenegger',): 1,\n",
       "         ('Segal',): 1,\n",
       "         ('Steven',): 1,\n",
       "         ('The',): 1,\n",
       "         ('Van',): 1,\n",
       "         ('``',): 1,\n",
       "         ('a',): 1,\n",
       "         ('and',): 1,\n",
       "         ('be',): 1,\n",
       "         ('destined',): 1,\n",
       "         ('even',): 1,\n",
       "         ('going',): 1,\n",
       "         ('greater',): 1,\n",
       "         ('he',): 1,\n",
       "         ('is',): 1,\n",
       "         ('make',): 1,\n",
       "         ('new',): 1,\n",
       "         ('or',): 1,\n",
       "         ('splash',): 1,\n",
       "         ('than',): 1,\n",
       "         ('that',): 1,\n",
       "         ('the',): 1,\n",
       "         ('to',): 2})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def k_grams(tree,k):\n",
    "    assert type(k) is int\n",
    "    words = [\"<S>\"]+list(tree.leaves())+[\"</S>\"]\n",
    "    k_grams = [tuple(words[i:i+k]) for i in range(len(words)-k)]\n",
    "    return Counter(k_grams)\n",
    "\n",
    "k_grams(next(sst.train_reader())[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nb_classifier_with_crossvalidation(X,y):\n",
    "    model = sklearn.naive_bayes.MultinomialNB()\n",
    "    param_grid = {'alpha':[0.1, 0.5, 1.0, 2.0]}\n",
    "    return sst.fit_classifier_with_crossvalidation(X, y, model, 3, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <listcomp>.<lambda> at 0x12b679730> <function fit_nb_classifier_with_crossvalidation at 0x120045488>\n",
      "-----------------------------\n",
      "Best params {'alpha': 2.0}\n",
      "Best score: 0.515\n",
      "Accuracy: 0.578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.665     0.283     0.397       428\n",
      "   positive      0.555     0.863     0.675       444\n",
      "\n",
      "avg / total      0.609     0.578     0.539       872\n",
      "\n",
      "-----------------------------\n",
      "<function <listcomp>.<lambda> at 0x12b6796a8> <function fit_nb_classifier_with_crossvalidation at 0x120045488>\n",
      "-----------------------------\n",
      "Best params {'alpha': 2.0}\n",
      "Best score: 0.515\n",
      "Accuracy: 0.578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.665     0.283     0.397       428\n",
      "   positive      0.555     0.863     0.675       444\n",
      "\n",
      "avg / total      0.609     0.578     0.539       872\n",
      "\n",
      "-----------------------------\n",
      "<function <listcomp>.<lambda> at 0x12b679620> <function fit_nb_classifier_with_crossvalidation at 0x120045488>\n",
      "-----------------------------\n",
      "Best params {'alpha': 2.0}\n",
      "Best score: 0.515\n",
      "Accuracy: 0.578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.665     0.283     0.397       428\n",
      "   positive      0.555     0.863     0.675       444\n",
      "\n",
      "avg / total      0.609     0.578     0.539       872\n",
      "\n",
      "-----------------------------\n",
      "<function <listcomp>.<lambda> at 0x12b6792f0> <function fit_nb_classifier_with_crossvalidation at 0x120045488>\n",
      "-----------------------------\n",
      "Best params {'alpha': 2.0}\n",
      "Best score: 0.515\n",
      "Accuracy: 0.578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.665     0.283     0.397       428\n",
      "   positive      0.555     0.863     0.675       444\n",
      "\n",
      "avg / total      0.609     0.578     0.539       872\n",
      "\n",
      "-----------------------------\n",
      "<function <listcomp>.<lambda> at 0x12b6797b8> <function fit_nb_classifier_with_crossvalidation at 0x120045488>\n",
      "-----------------------------\n",
      "Best params {'alpha': 2.0}\n",
      "Best score: 0.515\n",
      "Accuracy: 0.578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.665     0.283     0.397       428\n",
      "   positive      0.555     0.863     0.675       444\n",
      "\n",
      "avg / total      0.609     0.578     0.539       872\n",
      "\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "gram_funcs = [lambda x: k_grams(x,i) for i in range(5)]\n",
    "classifier_funcs = [fit_nb_classifier_with_crossvalidation]\n",
    "for phi in gram_funcs:\n",
    "    for clf_func in classifier_funcs:\n",
    "        run_and_test(phi,clf_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Submission V_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsmdata_home = 'vsmdata'\n",
    "glove_home = os.path.join(vsmdata_home, 'glove.6B')\n",
    "\n",
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(glove_home, 'glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_leaves_phi(tree, lookup, np_func=np.sum):\n",
    "    \"\"\"Represent tree as a combination of the vector of its words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : nltk.Tree   \n",
    "    lookup : dict\n",
    "        From words to vectors.\n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise, \n",
    "        like `np.mean`, `np.sum`, or `np.prod`. The requirement is that \n",
    "        the function take `axis=0` as one of its arguments (to ensure\n",
    "        columnwise combination) and that it return a vector of a \n",
    "        fixed length, no matter what the size of the tree is.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `X.shape[1]`\n",
    "            \n",
    "    \"\"\"\n",
    "    dim = len(next(iter(lookup.values())))    \n",
    "    allvecs = np.array([lookup[w] for w in tree.leaves() if w in lookup])\n",
    "    if len(allvecs) == 0:\n",
    "        feats = np.zeros(dim)\n",
    "    else:       \n",
    "        feats = np_func(allvecs, axis=0)      \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_leaves_phi(tree, np_func=np.sum):\n",
    "    return vsm_leaves_phi(tree, glove_lookup, np_func=np_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_and_test(glove_leaves_phi,fit_tree_nn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
